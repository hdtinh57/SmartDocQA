import streamlit as st
import os
import tempfile
from core.rag_pipeline import RagPipeline

# --- Page Config ---
st.set_page_config(
    page_title="Smart Document Q&A System",
    page_icon="ğŸ¤–",
    layout="wide"
)

# --- Initialization ---
@st.cache_resource
def get_pipeline():
    # Cache the pipeline so models (like BGE-M3) are loaded only once
    return RagPipeline(use_local_vlm=False)

def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "pipeline" not in st.session_state:
        st.session_state.pipeline = get_pipeline()
    if "processed_files" not in st.session_state:
        st.session_state.processed_files = []

initialize_session_state()

# --- Custom CSS ---
st.markdown("""
<style>
    .reportview-container {
        margin-top: -2em;
    }
    .stDeployButton {display:none;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- Sidebar ---
with st.sidebar:
    st.title("ğŸ“‚ Upload Documents")
    st.markdown("há»— trá»£ Ä‘á»‹nh dáº¡ng PDF, PNG, JPG, JPEG.")
    
    uploaded_file = st.file_uploader("Upload file here", type=["pdf", "png", "jpg", "jpeg"])
    
    if st.button("Process Document", type="primary"):
        if uploaded_file is not None:
            if uploaded_file.name in st.session_state.processed_files:
                st.warning(f"File '{uploaded_file.name}' Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c Ä‘Ã³!")
            else:
                with st.spinner(f"Äang xá»­ lÃ½ {uploaded_file.name}... (OCR & VectorEmbedding)"):
                    # Save uploaded file to a temporary file
                    tfile = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1])
                    tfile.write(uploaded_file.read())
                    tfile.close()
                    
                    # Run Ingestion Pipeline
                    success = st.session_state.pipeline.ingest_document(
                        file_path=tfile.name, 
                        source_name=uploaded_file.name
                    )
                    
                    # Clean up temp file
                    os.unlink(tfile.name)
                    
                    if success:
                        st.session_state.processed_files.append(uploaded_file.name)
                        st.success(f"Xá»­ lÃ½ thÃ nh cÃ´ng: {uploaded_file.name}")
                    else:
                        st.error(f"Xá»­ lÃ½ tháº¥t báº¡i: {uploaded_file.name}. Vui lÃ²ng kiá»ƒm tra API Key (Mistral) trong file .env")
        else:
            st.error("Vui lÃ²ng upload má»™t file trÆ°á»›c khi nháº¥n Process.")
            
    st.divider()
    st.markdown("### ğŸ“š TÃ i liá»‡u Ä‘Ã£ lÆ°u")
    if st.session_state.processed_files:
        for file in st.session_state.processed_files:
            st.markdown(f"- ğŸ“„ `{file}`")
    else:
        st.markdown("*ChÆ°a cÃ³ tÃ i liá»‡u nÃ o*")

# --- Main Layout ---
st.title("ğŸ¤– Trá»£ lÃ½ AI há»i Ä‘Ã¡p tÃ i liá»‡u thÃ´ng minh (RAG)")
st.markdown("HÃ£y upload tÃ i liá»‡u á»Ÿ thanh bÃªn trÃ¡i (Sidebar) trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i Ä‘á»ƒ AI cÃ³ ngá»¯ cáº£nh.")

# Display chat messages from history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- Chat Input ---
if prompt := st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n vá» tÃ i liá»‡u..."):
    # Add user message to state and display
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate assistant response
    with st.chat_message("assistant"):
        with st.spinner("Äang tÃ¬m kiáº¿m thÃ´ng tin vÃ  suy nghÄ©..."):
            response = st.session_state.pipeline.ask(prompt)
            st.markdown(response)
            
    st.session_state.messages.append({"role": "assistant", "content": response})
